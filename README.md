# Shadow Knowledge Distillation

This project provides Pytorch implementation for Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer.

**Open source code and training logs are under preparation, please keep patience, thanks!**

![shake-poster](E:\github\SHAKE\poster\shake-poster.jpg)

**Training logs and models**: https://pan.baidu.com/s/1hXe6iTCFw8nD_heDpCh1ag  (shak) 

## Acknowledgements
This repo is partly based on the following repos, thank the authors a lot.
- [HobbitLong/RepDistiller](https://github.com/HobbitLong/RepDistiller)
- [Knowledge-Distillation-Zoo](https://github.com/AberHu/Knowledge-Distillation-Zoo)

## Citation
If you find that this project helps your research, please consider citing some of the following papers:

```

```

